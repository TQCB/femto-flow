{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merges: [('i', 's'), ('t', 'o'), ('r', 'e'), ('l', 'l'), ('h', 'is'), ('e', 'n'), ('w', 'o'), ('o', 'n'), ('i', 'n'), ('wo', 'r'), ('a', 't'), ('h', 'a'), ('T', 'his'), ('in', 'g'), ('l', 'd'), ('I', 't'), ('u', 'g'), ('i', 'e'), ('s', '.'), ('b', 'e'), ('t', 'e'), ('h', 'o'), ('ho', 'p'), ('hop', 'e'), ('g', 'e'), ('re', 's'), ('g', 'o'), ('a', 'll'), ('to', 'k'), ('tok', 'en'), ('token', 'i'), ('tokeni', 'z'), ('tokeniz', 'at'), ('tokenizat', 'i'), ('tokenizati', 'on'), ('t', 'his'), ('wor', 'k'), ('H', 'e'), ('He', 'll'), ('Hell', 'o'), ('wor', 'ld'), ('world', '!'), ('g', 're'), ('gre', 'at'), ('d', 'a'), ('da', 'y'), ('h', 'ug'), ('p', 'u'), ('pu', 'p'), ('pup', 'p'), ('pupp', 'ie'), ('puppie', 's.'), ('wo', 'u'), ('wou', 'ld'), ('s', 'ha'), ('sha', 'm'), ('sham', 'e'), ('n', 'o'), ('no', 't'), ('d', 'o'), ('s', 'o'), ('so', '.'), ('te', 'x'), ('tex', 't'), ('f', 'u'), ('fu', 'll'), ('o', 'f'), ('n', 'on')]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "class BytePairTokenizer:\n",
    "  def __init__(self, target_vocab_size):\n",
    "    self.target_vocab_size = target_vocab_size\n",
    "    self.merges = []\n",
    "  \n",
    "  def _compute_word_frequencies(self, corpus):\n",
    "    \"\"\"\n",
    "    Compute word frequencies from input corpus.\n",
    "    \"\"\"\n",
    "    return Counter([word for text in corpus for word in text.split()])\n",
    "  \n",
    "  def _initialize_tokens(self, word_frequency):\n",
    "    \"\"\"\n",
    "    Initialize tokens as single characters.\n",
    "    \"\"\"\n",
    "    return {word:list(word) for word in word_frequency.keys()}\n",
    "  \n",
    "  def _compute_pair_frequencies(self, tokens, word_frequency):\n",
    "    \"\"\"\n",
    "    Compute frequencies of adjacent token pairs.\n",
    "    \"\"\"\n",
    "    pair_freq = defaultdict(int)\n",
    "    for word, freq in word_frequency.items():\n",
    "      token = tokens[word]\n",
    "      for i in range(len(token) - 1):\n",
    "        pair = (token[i], token[i + 1])\n",
    "        pair_freq[pair] += freq\n",
    "    return pair_freq\n",
    "  \n",
    "  def _find_best_pair(self, pair_freq):\n",
    "    \"\"\"\n",
    "    Find most frequent pair in our tokenized corpus\n",
    "    \"\"\"\n",
    "    return max(pair_freq, key=pair_freq.get)\n",
    "\n",
    "  def _merge_tokens(self, tokens, pair):\n",
    "    \"\"\"\n",
    "    Replace occurences of a pair of tokens with the merged form.\n",
    "    \"\"\"\n",
    "    merged_tokens = {}\n",
    "    merge_str = ''.join(pair)\n",
    "    for word, token in tokens.items():\n",
    "      new_token = []\n",
    "      i = 0\n",
    "      while i < len(token):\n",
    "        if i < len(token) - 1 and (token[i], token[i + 1]) == pair:\n",
    "          new_token.append(merge_str)\n",
    "          i += 2\n",
    "        else:\n",
    "          new_token.append(token[i])\n",
    "          i += 1\n",
    "      merged_tokens[word] = new_token\n",
    "    return merged_tokens\n",
    "\n",
    "  def fit(self, corpus):\n",
    "    word_frequency = self._compute_word_frequencies(corpus)\n",
    "    tokens = self._initialize_tokens(word_frequency)\n",
    "\n",
    "    while len(self.merges) + len(set(\"\".join(tokens.keys()))) < self.target_vocab_size:\n",
    "      pair_frequency = self._compute_pair_frequencies(tokens, word_frequency)\n",
    "      best_pair = self._find_best_pair(pair_frequency)\n",
    "      if not best_pair:\n",
    "        break\n",
    "      self.merges.append(best_pair)\n",
    "      tokens = self._merge_tokens(tokens, best_pair)\n",
    "\n",
    "  def get_merge_length(self, merges):\n",
    "    '''\n",
    "    Get maximum length of token in a merge pair.\n",
    "    '''\n",
    "    merge_lengths = []\n",
    "    for merge in merges:\n",
    "      merge_lengths.append(max([len(c) for c in merge]))\n",
    "    return merge_lengths\n",
    "\n",
    "  def sort_merges(self, merges):\n",
    "    '''\n",
    "    Sorts a list of tuples of merges, to be in the order of the maximum\n",
    "    length of token they are merging.\n",
    "    '''\n",
    "    merge_lengths = bpe.get_merge_length(merges)\n",
    "    return [merge for length, merge in sorted(zip(merge_lengths, merges))]\n",
    "\n",
    "  def tokenize(self, corpus):\n",
    "    '''\n",
    "    Iteratively merge learnt pairs of tokens in a corpus.\n",
    "    '''\n",
    "    # Find max merge depth to iterate to\n",
    "    max_merge_depth = max(self.get_merge_length(self.merges))\n",
    "    \n",
    "    # Sort merges by depth needed to reach them\n",
    "    sorted_merges = self.sort_merges(self.merges)\n",
    "    \n",
    "    # Get merged string for each merge pair\n",
    "    merge_dict = {m:m[0]+m[1] for m in sorted_merges}\n",
    "\n",
    "    tokenized_corpus = []\n",
    "    for document in corpus:\n",
    "      document = list(document)\n",
    "      \n",
    "      # Re-iterate as many times as max merge depth to ensure we merge all pairs\n",
    "      merge_depth = 0\n",
    "      while merge_depth < max_merge_depth:\n",
    "        \n",
    "        # Iterate across entire document\n",
    "        i = 0\n",
    "        while i < len(document) - 1:\n",
    "          pair = (document[i], document[i+1])\n",
    "          if pair in merge_dict.keys():\n",
    "            merge_string = merge_dict.get(pair)\n",
    "            document[i] = merge_string\n",
    "            del document[i+1]\n",
    "          i += 1\n",
    "        merge_depth += 1\n",
    "        \n",
    "      tokenized_corpus.append(document)\n",
    "    return tokenized_corpus\n",
    "\n",
    "corpus = [\n",
    "  \"Hello world!\",\n",
    "  \"It is a great day to hug puppies.\",\n",
    "  \"It would be a shame to not do so.\",\n",
    "  \"This text is full of nonsense: I don't care!\",\n",
    "  \"I hope I have enough pair variety here to get an interesting result.\",\n",
    "  \"This project is going to be a challenge\",\n",
    "  \"This is all about tokenization.\",\n",
    "  \"I'm trying to make this tokenization algorithm work\",\n",
    "  \"I really I hope this works.\",\n",
    "]\n",
    "\n",
    "bpe = BytePairTokenizer(100)\n",
    "bpe.fit(corpus)\n",
    "print(f\"Merges: {bpe.merges}\")\n",
    "tokens = bpe.tokenize(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', ' ', 'world!'],\n",
       " ['It',\n",
       "  ' ',\n",
       "  'is',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'great',\n",
       "  ' ',\n",
       "  'day',\n",
       "  ' ',\n",
       "  'to',\n",
       "  ' ',\n",
       "  'hug',\n",
       "  ' ',\n",
       "  'puppies.'],\n",
       " ['It',\n",
       "  ' ',\n",
       "  'would',\n",
       "  ' ',\n",
       "  'be',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'shame',\n",
       "  ' ',\n",
       "  'to',\n",
       "  ' ',\n",
       "  'not',\n",
       "  ' ',\n",
       "  'do',\n",
       "  ' ',\n",
       "  'so.'],\n",
       " ['This',\n",
       "  ' ',\n",
       "  'text',\n",
       "  ' ',\n",
       "  'is',\n",
       "  ' ',\n",
       "  'full',\n",
       "  ' ',\n",
       "  'of',\n",
       "  ' ',\n",
       "  'no',\n",
       "  'n',\n",
       "  's',\n",
       "  'en',\n",
       "  's',\n",
       "  'e',\n",
       "  ':',\n",
       "  ' ',\n",
       "  'I',\n",
       "  ' ',\n",
       "  'do',\n",
       "  'n',\n",
       "  \"'\",\n",
       "  't',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'a',\n",
       "  're',\n",
       "  '!'],\n",
       " ['I',\n",
       "  ' ',\n",
       "  'hope',\n",
       "  ' ',\n",
       "  'I',\n",
       "  ' ',\n",
       "  'ha',\n",
       "  'v',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'en',\n",
       "  'o',\n",
       "  'ug',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'p',\n",
       "  'a',\n",
       "  'i',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'v',\n",
       "  'a',\n",
       "  'r',\n",
       "  'ie',\n",
       "  't',\n",
       "  'y',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  're',\n",
       "  ' ',\n",
       "  'to',\n",
       "  ' ',\n",
       "  'ge',\n",
       "  't',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'in',\n",
       "  'te',\n",
       "  'res',\n",
       "  't',\n",
       "  'ing',\n",
       "  ' ',\n",
       "  'res',\n",
       "  'u',\n",
       "  'l',\n",
       "  't',\n",
       "  '.'],\n",
       " ['This',\n",
       "  ' ',\n",
       "  'p',\n",
       "  'r',\n",
       "  'o',\n",
       "  'j',\n",
       "  'e',\n",
       "  'c',\n",
       "  't',\n",
       "  ' ',\n",
       "  'is',\n",
       "  ' ',\n",
       "  'go',\n",
       "  'ing',\n",
       "  ' ',\n",
       "  'to',\n",
       "  ' ',\n",
       "  'be',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'ha',\n",
       "  'll',\n",
       "  'en',\n",
       "  'ge'],\n",
       " ['This',\n",
       "  ' ',\n",
       "  'is',\n",
       "  ' ',\n",
       "  'all',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'b',\n",
       "  'o',\n",
       "  'u',\n",
       "  't',\n",
       "  ' ',\n",
       "  'tokenization',\n",
       "  '.'],\n",
       " ['I',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  ' ',\n",
       "  't',\n",
       "  'r',\n",
       "  'y',\n",
       "  'ing',\n",
       "  ' ',\n",
       "  'to',\n",
       "  ' ',\n",
       "  'm',\n",
       "  'a',\n",
       "  'k',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'this',\n",
       "  ' ',\n",
       "  'tokenization',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'l',\n",
       "  'go',\n",
       "  'r',\n",
       "  'i',\n",
       "  't',\n",
       "  'h',\n",
       "  'm',\n",
       "  ' ',\n",
       "  'work'],\n",
       " ['I',\n",
       "  ' ',\n",
       "  're',\n",
       "  'all',\n",
       "  'y',\n",
       "  ' ',\n",
       "  'I',\n",
       "  ' ',\n",
       "  'hope',\n",
       "  ' ',\n",
       "  'this',\n",
       "  ' ',\n",
       "  'work',\n",
       "  's.']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
