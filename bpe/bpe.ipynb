{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class BytePairTokenizer:\n",
    "  def __init__(self, target_vocab_size):\n",
    "    self.target_vocab_size = target_vocab_size\n",
    "    self.merges = []\n",
    "  \n",
    "  def _compute_word_frequencies(self, corpus):\n",
    "    \"\"\"Compute word frequencies from input corpus.\"\"\"\n",
    "    return Counter(\" \".join(corpus).split())\n",
    "  \n",
    "  def _initialize_tokens(self, word_frequency):\n",
    "    \"\"\"Initialize tokens as single characters.\"\"\"\n",
    "    return {word:list(word) for word in word_frequency}\n",
    "  \n",
    "  def _compute_pair_frequencies(self, tokens, word_frequency):\n",
    "    \"\"\"Compute frequencies of adjacent token pairs.\"\"\"\n",
    "    pair_freq = Counter()\n",
    "    for word, freq in word_frequency.items():\n",
    "      token = tokens[word]\n",
    "      for i in range(len(token) - 1):\n",
    "        pair_freq[tuple(token[i:i+2])] += freq\n",
    "    return pair_freq\n",
    "  \n",
    "  def _find_best_pair(self, pair_freq):\n",
    "    \"\"\"Find most frequent pair in our tokenized corpus\"\"\"\n",
    "    return pair_freq.most_common(1)[0][0] if pair_freq else None\n",
    "\n",
    "  def _merge_tokens(self, tokens, pair):\n",
    "    \"\"\"Replace occurences of a pair of tokens with the merged form.\"\"\"\n",
    "    merged_tokens = {}\n",
    "    merge_str = ''.join(pair)\n",
    "    for word, token in tokens.items():\n",
    "      new_token = []\n",
    "      i = 0\n",
    "      while i < len(token):\n",
    "        if i < len(token) - 1 and tuple(token[i:i+2]) == pair:\n",
    "          new_token.append(merge_str)\n",
    "          i += 2\n",
    "        else:\n",
    "          new_token.append(token[i])\n",
    "          i += 1\n",
    "      merged_tokens[word] = new_token\n",
    "    return merged_tokens\n",
    "\n",
    "  def fit(self, corpus):\n",
    "    word_frequency = self._compute_word_frequencies(corpus)\n",
    "    tokens = self._initialize_tokens(word_frequency)\n",
    "\n",
    "    while len(self.merges) + len(set(\"\".join(word_frequency.keys()))) < self.target_vocab_size:\n",
    "      pair_frequency = self._compute_pair_frequencies(tokens, word_frequency)\n",
    "      best_pair = self._find_best_pair(pair_frequency)\n",
    "      if not best_pair:\n",
    "        break\n",
    "      self.merges.append(best_pair)\n",
    "      tokens = self._merge_tokens(tokens, best_pair)\n",
    "  \n",
    "  def transform(self, corpus):\n",
    "    \"\"\"Iteratively merge learnt pairs of tokens in a corpus.\"\"\"\n",
    "    merge_dict = {m:m[0]+m[1] for m in self.merges}\n",
    "\n",
    "    tokenized_corpus = []\n",
    "    for document in corpus:\n",
    "\n",
    "        document = list(document)\n",
    "\n",
    "        for merge_pair, merged_token in merge_dict.items():\n",
    "          i = 0\n",
    "          while i < len(document) - 1:\n",
    "            if tuple(document[i:i + len(merge_pair)]) == merge_pair:\n",
    "              document = document[:i] + [merged_token] + document[i + len(merge_pair):]\n",
    "            i+=1\n",
    "        \n",
    "        tokenized_corpus.append(document)\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorizer:\n",
    "  def __init__(self):\n",
    "    self.vocabulary = {}\n",
    "    self.inverse_vocabulary = {}\n",
    "  \n",
    "  def fit(self, tokenized_corpus):\n",
    "    tokens = [tok for doc in tokenized_corpus for tok in doc]\n",
    "    token_frequency = Counter(tokens).most_common()\n",
    "\n",
    "    integer = 1\n",
    "\n",
    "    for token, _ in token_frequency:\n",
    "      self.vocabulary[integer] = token\n",
    "      self.inverse_vocabulary[token] = integer\n",
    "      integer += 1\n",
    "  \n",
    "  def transform(self, tokens):\n",
    "    vectorized_corpus = []\n",
    "\n",
    "    for doc in tokens:\n",
    "      vectorized_document = []\n",
    "\n",
    "      for tok in doc:\n",
    "        vectorized_document.append(self.inverse_vocabulary[tok])\n",
    "\n",
    "      vectorized_corpus.append(vectorized_document)\n",
    "\n",
    "    return vectorized_corpus\n",
    "  \n",
    "  def fit_transform(self, tokens):\n",
    "    self.fit(tokens)\n",
    "    return self.transform(tokens)\n",
    "  \n",
    "  def inverse_transform(self, tokens):\n",
    "    tokenized_corpus = []\n",
    "    for doc in tokens:\n",
    "      tokenized_document = []\n",
    "      for tok in doc:\n",
    "        tokenized_document.append(self.vocabulary[tok])\n",
    "      tokenized_corpus.append(tokenized_document)\n",
    "    return tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37, 1, 38], [18, 1, 8, 1, 2, 1, 39, 1, 40, 1, 5, 1, 41, 1, 42], [18, 1, 43, 1, 19, 1, 2, 1, 44, 1, 5, 1, 45, 1, 46, 1, 47], [9, 1, 48, 1, 8, 1, 49, 1, 50, 1, 51, 20, 10, 20, 6, 52, 1, 4, 1, 53, 54, 21, 3, 1, 11, 2, 12, 55], [4, 1, 22, 1, 4, 1, 23, 24, 6, 1, 10, 13, 56, 14, 1, 25, 2, 26, 7, 1, 24, 2, 7, 57, 3, 15, 1, 14, 6, 12, 1, 5, 1, 27, 3, 1, 2, 58, 1, 59, 60, 28, 3, 16, 1, 28, 29, 30, 3, 31], [9, 1, 25, 7, 13, 61, 6, 11, 3, 1, 8, 1, 32, 16, 1, 5, 1, 19, 1, 2, 1, 11, 23, 62, 10, 27], [9, 1, 8, 1, 33, 1, 2, 63, 13, 29, 3, 1, 34, 31], [4, 21, 17, 1, 3, 7, 15, 16, 1, 5, 1, 17, 2, 64, 6, 1, 35, 1, 34, 1, 2, 30, 32, 7, 26, 3, 14, 17, 1, 36], [4, 1, 12, 33, 15, 1, 4, 1, 22, 1, 35, 1, 36, 65]] \n",
      " [['Hello', ' ', 'world!'], ['It', ' ', 'is', ' ', 'a', ' ', 'great', ' ', 'day', ' ', 'to', ' ', 'hug', ' ', 'puppies.'], ['It', ' ', 'would', ' ', 'be', ' ', 'a', ' ', 'shame', ' ', 'to', ' ', 'not', ' ', 'do', ' ', 'so.'], ['This', ' ', 'text', ' ', 'is', ' ', 'full', ' ', 'of', ' ', 'non', 's', 'en', 's', 'e', ':', ' ', 'I', ' ', 'd', 'on', \"'\", 't', ' ', 'c', 'a', 're', '!'], ['I', ' ', 'hope', ' ', 'I', ' ', 'ha', 'v', 'e', ' ', 'en', 'o', 'ug', 'h', ' ', 'p', 'a', 'i', 'r', ' ', 'v', 'a', 'r', 'ie', 't', 'y', ' ', 'h', 'e', 're', ' ', 'to', ' ', 'ge', 't', ' ', 'a', 'n', ' ', 'in', 'te', 'res', 't', 'ing', ' ', 'res', 'u', 'l', 't', '.'], ['This', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'is', ' ', 'go', 'ing', ' ', 'to', ' ', 'be', ' ', 'a', ' ', 'c', 'ha', 'll', 'en', 'ge'], ['This', ' ', 'is', ' ', 'all', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'tokenization', '.'], ['I', \"'\", 'm', ' ', 't', 'r', 'y', 'ing', ' ', 'to', ' ', 'm', 'a', 'k', 'e', ' ', 'this', ' ', 'tokenization', ' ', 'a', 'l', 'go', 'r', 'i', 't', 'h', 'm', ' ', 'work'], ['I', ' ', 're', 'all', 'y', ' ', 'I', ' ', 'hope', ' ', 'this', ' ', 'work', 's.']]\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "  \"Hello world!\",\n",
    "  \"It is a great day to hug puppies.\",\n",
    "  \"It would be a shame to not do so.\",\n",
    "  \"This text is full of nonsense: I don't care!\",\n",
    "  \"I hope I have enough pair variety here to get an interesting result.\",\n",
    "  \"This project is going to be a challenge\",\n",
    "  \"This is all about tokenization.\",\n",
    "  \"I'm trying to make this tokenization algorithm work\",\n",
    "  \"I really I hope this works.\",\n",
    "]\n",
    "\n",
    "tokenizer = BytePairTokenizer(100)\n",
    "vectorizer = Vectorizer()\n",
    "\n",
    "tokenizer.fit(corpus)\n",
    "tokens = tokenizer.transform(corpus)\n",
    "sequence = vectorizer.fit_transform(tokens)\n",
    "\n",
    "print(\n",
    "  sequence,\n",
    "  '\\n',\n",
    "  vectorizer.inverse_transform(sequence)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratchnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
