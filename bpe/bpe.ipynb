{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class BytePairTokenizer:\n",
    "  def __init__(self, target_vocab_size):\n",
    "    self.target_vocab_size = target_vocab_size\n",
    "    self.merges = []\n",
    "  \n",
    "  def _compute_word_frequencies(self, corpus):\n",
    "    \"\"\"Compute word frequencies from input corpus.\"\"\"\n",
    "    return Counter(\" \".join(corpus).split())\n",
    "  \n",
    "  def _initialize_tokens(self, word_frequency):\n",
    "    \"\"\"Initialize tokens as single characters.\"\"\"\n",
    "    return {word:list(word) for word in word_frequency}\n",
    "  \n",
    "  def _compute_pair_frequencies(self, tokens, word_frequency):\n",
    "    \"\"\"Compute frequencies of adjacent token pairs.\"\"\"\n",
    "    pair_freq = Counter()\n",
    "    for word, freq in word_frequency.items():\n",
    "      token = tokens[word]\n",
    "      for i in range(len(token) - 1):\n",
    "        pair_freq[tuple(token[i:i+2])] += freq\n",
    "    return pair_freq\n",
    "  \n",
    "  def _find_best_pair(self, pair_freq):\n",
    "    \"\"\"Find most frequent pair in our tokenized corpus\"\"\"\n",
    "    if pair_freq:\n",
    "      return pair_freq.most_common(1)[0][0]\n",
    "    return None\n",
    "\n",
    "  def _merge_tokens(self, tokens, pair):\n",
    "    \"\"\"Replace occurences of a pair of tokens with the merged form.\"\"\"\n",
    "    merged_tokens = {}\n",
    "    merge_str = ''.join(pair)\n",
    "    for word, token in tokens.items():\n",
    "      new_token = []\n",
    "      i = 0\n",
    "      while i < len(token):\n",
    "        if i < len(token) - 1 and tuple(token[i:i+2]) == pair:\n",
    "          new_token.append(merge_str)\n",
    "          i += 2\n",
    "        else:\n",
    "          new_token.append(token[i])\n",
    "          i += 1\n",
    "      merged_tokens[word] = new_token\n",
    "    return merged_tokens\n",
    "\n",
    "  def fit(self, corpus):\n",
    "    word_frequency = self._compute_word_frequencies(corpus)\n",
    "    tokens = self._initialize_tokens(word_frequency)\n",
    "\n",
    "    while len(self.merges) + len(set(\"\".join(tokens.keys()))) < self.target_vocab_size:\n",
    "      pair_frequency = self._compute_pair_frequencies(tokens, word_frequency)\n",
    "      best_pair = self._find_best_pair(pair_frequency)\n",
    "      if not best_pair:\n",
    "        break\n",
    "      self.merges.append(best_pair)\n",
    "      tokens = self._merge_tokens(tokens, best_pair)\n",
    "\n",
    "  def get_merge_length(self, merges):\n",
    "    '''\n",
    "    Get maximum length of token in a merge pair.\n",
    "    '''\n",
    "    merge_lengths = []\n",
    "    for merge in merges:\n",
    "      merge_lengths.append(max([len(c) for c in merge]))\n",
    "    return merge_lengths\n",
    "\n",
    "  def sort_merges(self, merges):\n",
    "    '''\n",
    "    Sorts a list of tuples of merges, to be in the order of the maximum\n",
    "    length of token they are merging.\n",
    "    '''\n",
    "    merge_lengths = self.get_merge_length(merges)\n",
    "    return [merge for _, merge in sorted(zip(merge_lengths, merges))]\n",
    "  \n",
    "  def tokenize(self, corpus):\n",
    "      \"\"\"Iteratively merge learnt pairs of tokens in a corpus.\"\"\"\n",
    "\n",
    "      merge_dict = {m:m[0]+m[1] for m in self.merges}\n",
    "\n",
    "      tokenized_corpus = []\n",
    "      for document in corpus:\n",
    "\n",
    "          document = list(document)\n",
    "\n",
    "          for merge_pair, merged_token in merge_dict.items():\n",
    "            i = 0\n",
    "            while i < len(document) - 1:\n",
    "              if tuple(document[i:i + len(merge_pair)]) == merge_pair:\n",
    "                document = document[:i] + [merged_token] + document[i + len(merge_pair):]\n",
    "              i+=1\n",
    "          \n",
    "          tokenized_corpus.append(document)\n",
    "      return tokenized_corpus\n",
    "\n",
    "corpus = [\n",
    "  \"Hello world!\",\n",
    "  \"It is a great day to hug puppies.\",\n",
    "  \"It would be a shame to not do so.\",\n",
    "  \"This text is full of nonsense: I don't care!\",\n",
    "  \"I hope I have enough pair variety here to get an interesting result.\",\n",
    "  \"This project is going to be a challenge\",\n",
    "  \"This is all about tokenization.\",\n",
    "  \"I'm trying to make this tokenization algorithm work\",\n",
    "  \"I really I hope this works.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello', ' ', 'world!'], ['It', ' ', 'is', ' ', 'a', ' ', 'great', ' ', 'day', ' ', 'to', ' ', 'hug', ' ', 'puppies.'], ['It', ' ', 'would', ' ', 'be', ' ', 'a', ' ', 'shame', ' ', 'to', ' ', 'not', ' ', 'do', ' ', 'so.'], ['This', ' ', 'text', ' ', 'is', ' ', 'full', ' ', 'of', ' ', 'non', 's', 'en', 's', 'e', ':', ' ', 'I', ' ', 'd', 'on', \"'\", 't', ' ', 'c', 'a', 're', '!'], ['I', ' ', 'hope', ' ', 'I', ' ', 'ha', 'v', 'e', ' ', 'en', 'o', 'ug', 'h', ' ', 'p', 'a', 'i', 'r', ' ', 'v', 'a', 'r', 'ie', 't', 'y', ' ', 'h', 'e', 're', ' ', 'to', ' ', 'ge', 't', ' ', 'a', 'n', ' ', 'in', 'te', 'res', 't', 'ing', ' ', 'res', 'u', 'l', 't', '.'], ['This', ' ', 'p', 'r', 'o', 'j', 'e', 'c', 't', ' ', 'is', ' ', 'go', 'ing', ' ', 'to', ' ', 'be', ' ', 'a', ' ', 'c', 'ha', 'll', 'en', 'ge'], ['This', ' ', 'is', ' ', 'all', ' ', 'a', 'b', 'o', 'u', 't', ' ', 'tokenization', '.'], ['I', \"'\", 'm', ' ', 't', 'r', 'y', 'ing', ' ', 'to', ' ', 'm', 'a', 'k', 'e', ' ', 'this', ' ', 'tokenization', ' ', 'a', 'l', 'go', 'r', 'i', 't', 'h', 'm', ' ', 'work'], ['I', ' ', 're', 'all', 'y', ' ', 'I', ' ', 'hope', ' ', 'this', ' ', 'work', 's.']]\n"
     ]
    }
   ],
   "source": [
    "bpe = BytePairTokenizer(100)\n",
    "bpe.fit(corpus)\n",
    "tokens = bpe.tokenize(corpus)\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
